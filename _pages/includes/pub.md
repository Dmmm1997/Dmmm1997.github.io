# üìù Publications 

<!---------------------------------------------------------------------------------------------->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2025</div><img src='images/publications/2024_TPAMI_FGVTP/main.svg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`TPAMI 2025` Fine-Grained Visual Text Prompting \\
**Lingfeng Yang**, Xiang Li, Yueze Wang, Xinlong Wang, Jian Yang

[**Paper**](https://ieeexplore.ieee.org/document/10763465)
 | [**Code** ![](https://img.shields.io/github/stars/ylingfeng/FGVP?style=social)](https://github.com/ylingfeng/FGVP)


- Proposes fine-grained multimodal prompting to enhance large multimodal models‚Äô localization and grounding capability, thereby boosting referring comprehension performance.
- Our work has been adopted by the research group of Prof. Philip H. S. Torr (Oxford University, **Marr Prize laureate**), who employed the proposed Fine-Grained Visual Prompting (FGVTP) as the core target extractor in their weakly supervised referring segmentation framework.
- Our work has inspired subsequent studies and has been applied to multiple domains, including **Egocentric Action Recognition** and **Compositional Action Recognition** for **embodied intelligence perception**.


</div>
</div>

<!---------------------------------------------------------------------------------------------->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023</div><img src='images/publications/2023_NeurIPS_FGVP/main.svg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`NeurIPS 2023` Fine-Grained Visual Prompting \\
**Lingfeng Yang**, Yueze Wang, Xiang Li, Xinlong Wang, Jian Yang

[**Paper**](https://proceedings.neurips.cc/paper_files/paper/2023/file/4e9fa6e716940a7cfc60c46e6f702f52-Paper-Conference.pdf)
 | [**Code** ![](https://img.shields.io/github/stars/ylingfeng/FGVP?style=social)](https://github.com/ylingfeng/FGVP)
 | [**‰∏≠ÊñáËß£ËØª**](https://mp.weixin.qq.com/s?search_click_id=10536340093298438394-1705732863737-1260009527&__biz=MzUxMDE4MzAzOA==&mid=2247714099&idx=1&sn=efe4d92ccece149d624d44a19f75404f&chksm=f8982f6663c6f4103967040294490fb7419803ceb6b54f2e79de728104a1858ad03f011d3fb8&scene=7&subscene=90&sessionid=1705732839&clicktime=1705732863&enterid=1705732863&ascene=65&fasttmpl_type=0&fasttmpl_fullversion=7038836-zh_CN-zip&fasttmpl_flag=0&realreporttime=1705732863790&devicetype=android-33&version=28002d3b&nettype=WIFI&abtest_cookie=AAACAA%3D%3D&lang=zh_CN&countrycode=CN&exportkey=n_ChQIAhIQTY3OsEwNdtlJy0RxUEMZyxLcAQIE97dBBAEAAAAAAJ%2F5F8UMLd0AAAAOpnltbLcz9gKNyK89dVj0fDJfc0iQOozTOSv7wroTFtyx6pfMLQW9ACiiUD2XPYTJToJQxVNxvrF5tAIC8R0SbOS35hwJULATy64LUtXxEgmsCoz6Cqv01v%2B25HzaDWybt6vi82M5Lad5HaUdHZAgh4kTKQl9Lri9nQxeptfavWT7F389xOk%2BXh7B4nHuFz%2BeaRdMmZf6lLv3kLpf10%2BJykklCd3SfLyGkE68DPfh1hmFhext2v%2BZTOids%2B0QavnzY7GPOQE%3D&pass_ticket=h3SZ5GzwbdiBvmS547xoTsCldqEAFLvligHaiMY%2BXuAaSiUHNNO2iFTVImHJqOpfAucoZ0LcWe34Hs99pbaVbA%3D%3D&wx_header=3&poc_token=HEYkVWijKQwOws52LqNI8BFkPicAMjsAOeCl7vHt)
 | [**‰∏≠ÊñáËßÜÈ¢ë**](https://www.bilibili.com/video/BV1qw411873s/?spm_id_from=333.999.0.0&vd_source=55bfc02adba971ea9a2c7d47e95180cc)


- Propose a specific visual prompting technique that enhances referring expression comprehension by highlighting regions of interest through background blurring based on fine-grained segmentation.
- Maintains faster inference speed in the trade-off while achieving more than a **5-point improvement** over state-of-the-art methods.


</div>
</div>

<!---------------------------------------------------------------------------------------------->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2022 Spotlight</div><img src='images/publications/2022_NeurIPS_RM/main.svg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`NeurIPS 2022` RecursiveMix: Mixed Learning with History (<strong style="color:red;">Spotlight</strong>, Top 12.8%) \\
**Lingfeng Yang**, Xiang Li, Borui Zhao, Renjie Song, Jian Yang

[**Paper**](https://proceedings.neurips.cc/paper_files/paper/2022/file/37e44c4b5321605735be9761f9b758fc-Paper-Conference.pdf)
 | [**Code** ![](https://img.shields.io/github/stars/implus/RecursiveMix-pytorch?style=social)](https://github.com/implus/RecursiveMix-pytorch)


- Propose a simple yet effective mixed-data augmentation technique for image classification.
- Enhance model pretraining performance for object detection and semantic segmentation tasks.


</div>
</div>

<!---------------------------------------------------------------------------------------------->
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2022 Oral</div><img src='images/publications/2022_CVPR_DynamicMLP/main.svg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`CVPR 2022` Dynamic MLP for Fine-Grained Image Classification by Leveraging Geographical and Temporal Information (<strong style="color:red;">Oral</strong>, Top 3.3%)  \\
**Lingfeng Yang**, Xiang Li, Renjie Song, Borui Zhao, Juntian Tao, Shihao Zhou, Jiajun Liang, Jian Yang

[**Paper**](https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Dynamic_MLP_for_Fine-Grained_Image_Classification_by_Leveraging_Geographical_and_CVPR_2022_paper.pdf)
| [**Code** ![](https://img.shields.io/github/stars/ylingfeng/DynamicMLP?style=social)](https://github.com/ylingfeng/DynamicMLP)


- Proposed a dynamic MLP fusion framework for fine-grained image classification by incorporating geo-temporal information.
- Improved classification accuracy on multiple fine-grained datasets.


</div>
</div>
