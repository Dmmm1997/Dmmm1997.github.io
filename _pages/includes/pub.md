# üìù Publications

## Referring Video Object Segmentation (RefVOS)


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2025</div><img src='images/publications/Arxiv-MomentSeg/momentseg.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`Arxiv 2025`  MomentSeg: Moment-Centric Sampling for Enhanced Video Pixel Understanding \\
**Ming Dai**, Sen Yang, Boqiang Duan, Wankou Yang, Jingdong Wang

[**Paper**](https://arxiv.org/abs/2510.09274)
 | [**Code** ![](https://img.shields.io/github/stars/Dmmm1997/MomentSeg?style=social)](https://github.com/Dmmm1997/MomentSeg)
 | [**Project**](https://dmmm1997.github.io/momentseg/)

 > **Highlights**: **MomentSeg** unifies **temporal grounding** and **segmentation**, enabling key-frame extraction *without* relying on any external models.  
In addition, we introduce a novel `[FIND]` token, which allows the model to perform temporal grounding *without* requiring any additional timestamp encoding.
</div>
</div>



<!---------------------------------------------------------------------------------------------->


## Visual Grounding (REC, RES, GREC, GRES)

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TPAMI 2025</div><img src='images/publications/TPAMI-InstanceVG/instancevg.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`TPAMI 2025`  Improving Generalized Visual Grounding with Instance-aware Joint Learning \\
**Ming Dai**, Wenxuan Cheng, Jiang-jiang Liu, Lingfeng Yang, Zhenhua Feng, Wankou Yang, Jingdong Wang

[**Paper**](https://ieeexplore.ieee.org/document/11153716)
 | [**Code** ![](https://img.shields.io/github/stars/Dmmm1997/InstanceVG?style=social)](https://github.com/Dmmm1997/InstanceVG)
 | [**‰∏≠ÊñáËß£ËØª**](https://zhuanlan.zhihu.com/p/1951971188877275922)

 > **Highlights**: **InstanceVG** supports **instance-level** referring segmentation across general scenarios (no/single/multiple targets). It also provides consistent prediction across `point`, `box`, and `mask` inputs.

</div>
</div>

<!---------------------------------------------------------------------------------------------->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/publications/ICCV2025-PropVG/framework.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`ICCV 2025`  PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination \\
**Ming Dai**, Wenxuan Cheng, Jiedong Zhuang, Jiang-jiang Liu, Hongshen Zhao, Zhenhua Feng, Wankou Yang

[**Paper**](https://arxiv.org/abs/2509.04833)
 | [**Code** ![](https://img.shields.io/github/stars/Dmmm1997/PropVG?style=social)](https://github.com/Dmmm1997/PropVG)
 | [**‰∏≠ÊñáËß£ËØª**](https://zhuanlan.zhihu.com/p/1948344988695007969)

 > **Highlights**: **PropVG** achieves **end-to-end two-stage** visual grounding, overcoming the traditional drawbacks of previous two-stage approaches that *relied on external detectors* and were often associated with *slow inference and limited performance*.

</div>
</div>



<!---------------------------------------------------------------------------------------------->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/publications/ICCV2025-DeRIS/framework.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`ICCV 2025` DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy \\
**Ming Dai**, Wenxuan Cheng, Jiang-jiang Liu, Sen Yang, Wenxiao Cai, Yanpeng Sun, Wankou Yang

[**Paper**](https://arxiv.org/abs/2507.01738)
 | [**Code** ![img](https://img.shields.io/github/stars/Dmmm1997/DeRIS?style=social)](https://github.com/Dmmm1997/DeRIS)

> **Highlights**: **DeRIS** analyzes a key bottleneck in visual grounding‚Äî**Cognition**. It decouples the VG task into *perception* and *cognition* components, and integrates them effectively through a loopback synergy mechanism.

</div>
</div>


<!---------------------------------------------------------------------------------------------->

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2025 (Selected as Oral)</div><img src='images/publications/AAAI2025-C3VG/framework.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`AAAI 2025` Multi-task Visual Grounding with Coarse-to-Fine Consistency Constraints \\
**Ming Dai**, Jian Li, Jiedong Zhuang, Xian Zhang, Wankou Yang

[**Paper**](https://arxiv.org/pdf/2501.06710)
 | [**Code** ![](https://img.shields.io/github/stars/Dmmm1997/C3VG?style=social)](https://github.com/Dmmm1997/C3VG)

> **Highlights**: **C3VG** investigates the **consistency** prediction problem in REC and RIS, introducing a **coarse-to-fine** architecture that enforces consistency through both `implicit` and `explicit` constraints.

</div>
</div>




<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/publications/NeurIPS2025-SimVG/framework.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`NeurIPS 2024` SimVG: A Simple Framework for Visual Grounding with Decoupled Multi-modal Fusion \\
**Ming Dai**, Lingfeng Yang, Yihao Xu, Zhenhua Feng, Wankou Yang


[**Paper**](https://arxiv.org/abs/2409.17531)
| [**Code** ![](https://img.shields.io/github/stars/Dmmm1997/SimVG?style=social)](https://github.com/Dmmm1997/SimVG)
| [**‰∏≠ÊñáËß£ËØª**](https://zhuanlan.zhihu.com/p/818287010)

> **Highlights**: **SimVG** explores the importance of **multi-modal understanding** for the VG task, proposing a **simple** yet effective framework. It also adopts a **synchronized distillation** learning strategy between the teacher and student branches, enhancing the performance of the student branch.
</div>


## Cross-View Geo-Localization


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv 2024</div><img src='images/publications/Arxiv-DRL/main.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`Arxiv 2024` Drone Referring Localization: An Efficient Heterogeneous Spatial Feature Interaction Method For UAV Self-Localization \\
**Ming Dai**, Enhui Zheng, Jiahao Chen, Lei Qi, Zhenhua Feng, Wankou Yang


[**Paper**](https://arxiv.org/pdf/2208.06561)
| [**Code** ![](https://img.shields.io/github/stars/Dmmm1997/DRL?style=social)](https://github.com/Dmmm1997/DRL)

> **Highlights**: **DRL** adopts an **end-to-end** training and inference paradigm to address common issues in image-retrieval-based UAV self-localization, including *complex preprocessing*, *inherent localization errors*, and *slow inference*.

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TIP 2023</div><img src='images/publications/TIP2023-DenseUAV/main.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`TIP 2023` Vision-Based UAV Self-Positioning in Low-Altitude Urban Environments \\
**Ming Dai**, Enhui Zheng, Zhenhua Feng, Jiedong Zhuang, Wankou Yang


[**Paper**](https://arxiv.org/abs/2201.09201)
| [**Code** ![](https://img.shields.io/github/stars/Dmmm1997/DenseUAV?style=social)](https://github.com/Dmmm1997/DenseUAV)
| [**‰∏≠ÊñáËß£ËØª**](https://zhuanlan.zhihu.com/p/673051338)

> **Highlights**: **DenseUAV** introduces a **real-world** sampled dataset for vision-based UAV self-localization and provides a comprehensive benchmark for the task.


</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">TCSVT 2021</div><img src='images/publications/TCSVT2021-FSRA/framework.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

`TCSVT 2021` A Transformer-Based Feature Segmentation and Region Alignment Method For UAV-View Geo-Localization \\
**Ming Dai**, Jianhong Hu, Jiedong Zhuang, Enhui Zheng


[**Paper**](https://arxiv.org/abs/2201.09206)
| [**Code** ![](https://img.shields.io/github/stars/Dmmm1997/FSRA?style=social)](https://github.com/Dmmm1997/FSRA)


> **Highlights**: **FSRA** is the *first* successful application of Transformer models to cross-view geo-localization. It introduces an attention-map-based region partitioning and alignment strategy that alleviates performance degradation caused by viewpoint shifts.

</div>
</div>




